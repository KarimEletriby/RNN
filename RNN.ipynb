{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiBHFkoE0OR8",
        "outputId": "30d01509-5efb-4591-92df-5e4e4e078c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 10, Loss: 0.1016\n",
            "Epoch 20, Loss: 0.0140\n",
            "Epoch 30, Loss: 0.0053\n",
            "Epoch 40, Loss: 0.0033\n",
            "Epoch 50, Loss: 0.0026\n",
            "Epoch 60, Loss: 0.0023\n",
            "Epoch 70, Loss: 0.0020\n",
            "Epoch 80, Loss: 0.0018\n",
            "Epoch 90, Loss: 0.0017\n",
            "Epoch 100, Loss: 0.0016\n",
            "\n",
            "Prediction Results:\n",
            "Input sequence: ['Ali', 'Adham', 'Nour']\n",
            "Predicted next word: Karim\n",
            "Actual next word: Karim\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "words = ['Ali', 'Adham', 'Nour', 'Karim']\n",
        "word_to_idx = {word: idx for idx, word in enumerate(words)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(words)}\n",
        "\n",
        "sequence = ['Ali', 'Adham', 'Nour']\n",
        "target = 'Karim'\n",
        "\n",
        "X = [word_to_idx[word] for word in sequence]\n",
        "y = word_to_idx[target]\n",
        "\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        output, hidden = self.rnn(embedded)\n",
        "        out = self.fc(output[:, -1, :])\n",
        "        return out\n",
        "\n",
        "vocab_size = len(words)\n",
        "embedding_dim = 10\n",
        "hidden_dim = 16\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "model = SimpleRNN(vocab_size, embedding_dim, hidden_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "X_train = torch.tensor([X], dtype=torch.long)\n",
        "y_train = torch.tensor([y], dtype=torch.long)\n",
        "\n",
        "print(\"Starting training...\")\n",
        "for epoch in range(epochs):\n",
        "    model.zero_grad()\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_input = torch.tensor([X], dtype=torch.long)\n",
        "    output = model(test_input)\n",
        "    predicted_idx = torch.argmax(output, dim=1).item()\n",
        "    predicted_word = idx_to_word[predicted_idx]\n",
        "\n",
        "print(\"\\nPrediction Results:\")\n",
        "print(f\"Input sequence: {sequence}\")\n",
        "print(f\"Predicted next word: {predicted_word}\")\n",
        "print(f\"Actual next word: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8ZUydMz0hO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}