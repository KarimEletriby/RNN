{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiBHFkoE0OR8",
        "outputId": "bce2fded-2d68-4afd-9779-42a3abb25ebd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.3887\n",
            "Epoch 10, Loss: 1.3123\n",
            "Epoch 20, Loss: 1.2400\n",
            "Epoch 30, Loss: 1.1717\n",
            "Epoch 40, Loss: 1.1071\n",
            "Epoch 50, Loss: 1.0462\n",
            "Epoch 60, Loss: 0.9889\n",
            "Epoch 70, Loss: 0.9349\n",
            "Epoch 80, Loss: 0.8841\n",
            "Epoch 90, Loss: 0.8363\n",
            "\n",
            "Prediction Results:\n",
            "Input sequence: ['Ali', 'Mohamed', 'Aya']\n",
            "Predicted next word: Karim\n",
            "Actual next word: Karim\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "words = ['Ali', 'Mohamed', 'Aya', 'Karim']\n",
        "word_to_idx = {word: i for i, word in enumerate(words)}\n",
        "idx_to_word = {i: word for i, word in enumerate(words)}\n",
        "\n",
        "sequence = ['Ali', 'Mohamed', 'Aya']\n",
        "target = 'Karim'\n",
        "X = [word_to_idx[word] for word in sequence]\n",
        "y = word_to_idx[target]\n",
        "\n",
        "vocab_size = len(words)\n",
        "embedding_dim = 10\n",
        "hidden_dim = 16\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "def init_weights(rows, cols):\n",
        "    return [[random.uniform(-0.1, 0.1) for _ in range(cols)] for _ in range(rows)]\n",
        "\n",
        "W_embed = init_weights(vocab_size, embedding_dim)\n",
        "W_hh = init_weights(hidden_dim, hidden_dim)\n",
        "W_xh = init_weights(hidden_dim, embedding_dim)  # Fixed dimensions\n",
        "b_h = [0.0] * hidden_dim\n",
        "W_hy = init_weights(vocab_size, hidden_dim)\n",
        "b_y = [0.0] * vocab_size\n",
        "\n",
        "def tanh(x):\n",
        "    return math.tanh(x)\n",
        "\n",
        "def softmax(x):\n",
        "    exp = [math.exp(i) for i in x]\n",
        "    sum_exp = sum(exp)\n",
        "    return [i/sum_exp for i in exp]\n",
        "\n",
        "def forward(input_seq):\n",
        "    h = [0.0] * hidden_dim\n",
        "    embeddings = [W_embed[x] for x in input_seq]\n",
        "\n",
        "    for emb in embeddings:\n",
        "        h_input = [sum(a*b for a,b in zip(emb, W_xh[i])) for i in range(hidden_dim)]\n",
        "        h_recur = [sum(a*b for a,b in zip(h, W_hh[i])) for i in range(hidden_dim)]\n",
        "        h = [tanh(h_input[i] + h_recur[i] + b_h[i]) for i in range(hidden_dim)]\n",
        "\n",
        "    y = [sum(h[i] * W_hy[j][i] for i in range(hidden_dim)) + b_y[j] for j in range(vocab_size)]\n",
        "    return softmax(y), h\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    probs, h = forward(X)\n",
        "    loss = -math.log(probs[y])\n",
        "\n",
        "    dy = probs.copy()\n",
        "    dy[y] -= 1\n",
        "\n",
        "    dW_hy = [[0.0]*hidden_dim for _ in range(vocab_size)]\n",
        "    for j in range(vocab_size):\n",
        "        for i in range(hidden_dim):\n",
        "            dW_hy[j][i] = h[i] * dy[j]\n",
        "\n",
        "    db_y = dy.copy()\n",
        "\n",
        "    dh = [sum(dy[j] * W_hy[j][i] for j in range(vocab_size)) for i in range(hidden_dim)]\n",
        "    dh = [dh[i] * (1 - h[i]**2) for i in range(hidden_dim)]\n",
        "\n",
        "    for i in range(hidden_dim):\n",
        "        for j in range(hidden_dim):\n",
        "            W_hh[i][j] -= learning_rate * dh[i] * h[j]\n",
        "\n",
        "    for i in range(hidden_dim):\n",
        "        for j in range(embedding_dim):\n",
        "            W_xh[i][j] -= learning_rate * dh[i] * W_embed[X[-1]][j]\n",
        "\n",
        "    for i in range(hidden_dim):\n",
        "        b_h[i] -= learning_rate * dh[i]\n",
        "\n",
        "    for j in range(vocab_size):\n",
        "        for i in range(hidden_dim):\n",
        "            W_hy[j][i] -= learning_rate * dW_hy[j][i]\n",
        "\n",
        "    for i in range(vocab_size):\n",
        "        b_y[i] -= learning_rate * db_y[i]\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "probs, _ = forward(X)\n",
        "predicted_idx = probs.index(max(probs))\n",
        "predicted_word = idx_to_word[predicted_idx]\n",
        "\n",
        "print(\"\\nPrediction Results:\")\n",
        "print(f\"Input sequence: {sequence}\")\n",
        "print(f\"Predicted next word: {predicted_word}\")\n",
        "print(f\"Actual next word: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8ZUydMz0hO0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}